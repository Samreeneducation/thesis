{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datacleaning preprocessing and sentiment.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samreeneducation/thesis/blob/master/datacleaning_preprocessing_and_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJhXyxbGKwbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIGWXCGcYG3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import textblob\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation \n",
        "from nltk.corpus import stopwords\n",
        "from textblob import  TextBlob\n",
        "import re\n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCPQEfnrYdZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"data_train_clean.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI8c_DBma1dB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgIZABF8a8j8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(clean_comment):\n",
        "    return ' '.join(re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ',clean_comment).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VyEWhF5e8h7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOzXCe54fot1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analyze_sentiment(clean_comment):\n",
        "    analysis = TextBlob(clean_comment)\n",
        "    if analysis.sentiment.polarity > 0:\n",
        "        return '0'\n",
        "    elif analysis.sentiment.polarity ==0:\n",
        "        return '0'\n",
        "    else:\n",
        "        return '1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7PZLg-gfuA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['cyberbully'] = df['clean_comment'].apply(lambda x: analyze_sentiment(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8CX4xyFhE6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwG_h52nitbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data cleaning and preprcessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7C06Nc8izwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from bs4 import BeautifulSoup\n",
        "example1 = BeautifulSoup(df.clean_comment[111698], 'lxml')\n",
        "#print example1.get_text()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69LgzVxci76D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop any rows which have any nans\n",
        "df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng8hCWDFjFUv",
        "colab_type": "text"
      },
      "source": [
        "First define the data cleaning function, include the above five data cleaning tasks, and then apply it to the entire data set. Word segmentation, stemming / morphization, stop words will be used later when creating matrices with count vectorizer and Tfidf vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSSYa7odjkw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "def tweet_cleaner(clean_comment):\n",
        "    soup = BeautifulSoup(\n",
        "       clean_comment, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lower_case = letters_only.lower()\n",
        "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
        "    # I will tokenize and join together to remove unneccessary white spaces\n",
        "    words = tok.tokenize(lower_case)\n",
        "    return (\" \".join(words)).strip()\n",
        "testing = df.clean_comment[0:111698]\n",
        "test_result = []\n",
        "for t in testing:\n",
        "    test_result.append(tweet_cleaner(t))\n",
        "test_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUbti33vkFZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "def tweet_cleaner( clean_comment):\n",
        "    soup = BeautifulSoup( clean_comment, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lower_case = letters_only.lower()\n",
        "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
        "    # I will tokenize and join together to remove unneccessary white spaces\n",
        "    words = tok.tokenize(lower_case)\n",
        "    return (\" \".join(words)).strip()\n",
        "testing = df. clean_comment[0:111698]\n",
        "test_result = []\n",
        "for t in testing:\n",
        "    test_result.append(tweet_cleaner(t))\n",
        "test_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhXlXL68kvFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing = df.clean_comment[0:111698]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oR1sf77k9WC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing.replace(u\"\\ufffd\", \"?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rah5id-JlDG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nums = [0,111698]\n",
        "print(\"Cleaning and parsing the tweets...\\n\")\n",
        "clean_tweet_texts = []\n",
        "for i in range(nums[0],nums[1]):\n",
        "    if( (i+1)%10000 == 0 ):\n",
        "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1]))                                                                    \n",
        "    clean_tweet_texts.append(tweet_cleaner(df['clean_comment'][i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzAh69Vil_6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0uzhPJZzvro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('datatrainclean.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbZbQoh8z__E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_csv = pd.read_csv('datatrainclean.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x105MKij0M06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df.to_csv('datatrainclean.csv',encoding='utf-8')\n",
        "csv = 'datatrainclean.csv'\n",
        "my_df = pd.read_csv(csv,index_col=0)\n",
        "my_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfRXnDF60jqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZUU_q4bvXJ2",
        "colab_type": "text"
      },
      "source": [
        "We remove the username in every tweet, so basically we can remove everything that begins with @ and we use regex to do it."
      ]
    }
  ]
}