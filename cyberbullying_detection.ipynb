{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cyberbullying detection.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samreeneducation/thesis/blob/master/cyberbullying_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIdQJGG5srWR",
        "colab_type": "text"
      },
      "source": [
        "Detecting Cyberbullying in comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtzKMvhylnKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
        "from time import time\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.externals import joblib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZaPkOS7p73R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NzJZWKnqsJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InQBK4Hss3wW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_scraped = pd.read_csv('data_test_clean.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH7ubw5ksm5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECSzchvqv13o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_public= pd.read_csv('datatrainclean.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVEaUy2BySQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_scraped.head(2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8Mjw5VMyYac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_public.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxxMdsL6zMm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for dropping any row from dataframe incase drop id from data frame\n",
        "df_scraped.drop_duplicates(inplace = True)\n",
        "df_scraped.drop('id',axis = 'columns', inplace = True)\n",
        "\n",
        "df_public.drop_duplicates(inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwTKOyulzbSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([df_scraped, df_public])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1RcpBwozkVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eCaz3bjzpTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (7,7))\n",
        "sorted_counts = df['toxic'].value_counts()\n",
        "plt.pie(sorted_counts, labels= sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n",
        "       autopct='%1.2f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 15}, shadow = True,\n",
        "        colors = sns.color_palette(\"Paired\")[7:])\n",
        "plt.text(x = -0.35, y = 0, s = 'toxic: {}'.format(df.shape[0]))\n",
        "\n",
        "plt.title('Distribution of toxic in the Dataset', fontsize = 16);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7gXOJiI8miI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (7,7))\n",
        "sorted_counts = df['severe_toxic'].value_counts()\n",
        "plt.pie(sorted_counts, labels= sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n",
        "       autopct='%1.2f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 15}, shadow = True,\n",
        "        colors = sns.color_palette(\"Paired\")[7:])\n",
        "plt.text(x = -0.35, y = 0, s = 'severe_toxic: {}'.format(df.shape[0]))\n",
        "\n",
        "plt.title('Distribution of severe_toxic in the Dataset', fontsize = 16);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihxljg7_9FLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZzOO0w8zeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (7,7))\n",
        "sorted_counts = df['obscene'].value_counts()\n",
        "plt.pie(sorted_counts, labels= sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n",
        "       autopct='%1.2f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 15}, shadow = True,\n",
        "        colors = sns.color_palette(\"Paired\")[7:])\n",
        "plt.text(x = -0.35, y = 0, s = 'obscene: {}'.format(df.shape[0]))\n",
        "\n",
        "plt.title('Distribution of obscene in the Dataset', fontsize = 16);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOUoEohe8DQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (7,7))\n",
        "sorted_counts = df['threat'].value_counts()\n",
        "plt.pie(sorted_counts, labels= sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n",
        "       autopct='%1.2f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 15}, shadow = True,\n",
        "        colors = sns.color_palette(\"Paired\")[7:])\n",
        "plt.text(x = -0.35, y = 0, s = 'threat: {}'.format(df.shape[0]))\n",
        "\n",
        "plt.title('Distribution of threat in the Dataset', fontsize = 16);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTCEYM1r9wkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (7,7))\n",
        "sorted_counts = df['insult'].value_counts()\n",
        "plt.pie(sorted_counts, labels= sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n",
        "       autopct='%1.2f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 15}, shadow = True,\n",
        "        colors = sns.color_palette(\"Paired\")[7:])\n",
        "plt.text(x = -0.35, y = 0, s = 'insult: {}'.format(df.shape[0]))\n",
        "\n",
        "plt.title('Distribution of insult in the Dataset', fontsize = 16);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EBLloxd-T2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (7,7))\n",
        "sorted_counts = df['identity_hate'].value_counts()\n",
        "plt.pie(sorted_counts, labels= sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n",
        "       autopct='%1.2f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 15}, shadow = True,\n",
        "        colors = sns.color_palette(\"Paired\")[7:])\n",
        "plt.text(x = -0.35, y = 0, s = 'identity_hate: {}'.format(df.shape[0]))\n",
        "\n",
        "plt.title('Distribution of identity_hate in the Dataset', fontsize = 16);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIY_SrMf0az1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['cyberbully'] = df.cyberbully.map({'bully': 1, 'notbully': 0})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2_KLKGz3MGZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Implementing a training and predicting Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5IKqZt84YL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Separate training and testing data:\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_comment'], \n",
        "                                                    df['cyberbully'], \n",
        "                                                    random_state=42)\n",
        "\n",
        "print('Number of rows in the total set: {}'.format(df.shape[0]))\n",
        "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
        "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQOLjbUq45ar",
        "colab_type": "text"
      },
      "source": [
        "since out puts are not being save in outide colab here am pasting it as text:\n",
        "Number of rows in the total set: 159571\n",
        "Number of rows in the training set: 119678\n",
        "Number of rows in the test set: 39893"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr0pJn6-5yfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the CountVectorizer method\n",
        "count_vector = CountVectorizer(stop_words = 'english', lowercase = True)\n",
        "\n",
        "# Fit the training data and then return the matrix\n",
        "training_data = count_vector.fit_transform(X_train)\n",
        "\n",
        "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
        "testing_data = count_vector.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILE_6JW56KVw",
        "colab_type": "text"
      },
      "source": [
        "Implement Pipeline:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfy8_DSD6LRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline(learner_list, X_train, y_train, X_test, y_test): \n",
        "    '''\n",
        "    inputs:\n",
        "       - learner: the learning algorithm to be trained and predicted on\n",
        "       - X_train: features training set\n",
        "       - y_train: income training set\n",
        "       - X_test: features testing set\n",
        "       - y_test: income testing set\n",
        "    '''\n",
        "    \n",
        "    # Get length of Training Data:\n",
        "    size = len(y_train)\n",
        "    \n",
        "    results = {}\n",
        "    final_results = []\n",
        "    \n",
        "    for learner in learner_list:\n",
        "        \n",
        "        # Store the learner name:\n",
        "        results['Algorithm'] = learner.__class__.__name__\n",
        "\n",
        "        # Fit the learner:\n",
        "        start = time() # Get start time\n",
        "        print(\"Training {}\".format(learner.__class__.__name__))\n",
        "        learner = learner.fit(X_train, y_train)\n",
        "        end = time() # Get end time\n",
        "\n",
        "        # Store the training time\n",
        "        results['Training Time'] = end - start\n",
        "\n",
        "        start = time() # Get start time\n",
        "        predictions_test = learner.predict(X_test)\n",
        "        predictions_train = learner.predict(X_train)\n",
        "        end = time() # Get end time\n",
        "\n",
        "        # Store the prediction time\n",
        "        results['Prediction Time'] = end - start\n",
        "\n",
        "        # Compute the Accuracy on Test Set\n",
        "        results['Accuracy: Test'] = accuracy_score(y_test, predictions_test)\n",
        "\n",
        "        # Compute the Accuracy on Training Set\n",
        "        results['Accuracy: Train'] = accuracy_score(y_train, predictions_train)\n",
        "\n",
        "        # Compute the F1 Score on Test Set\n",
        "        results['F1 Score: Test'] = f1_score(y_test, predictions_test)\n",
        "\n",
        "        # Compute the F1 Score on Training Set\n",
        "        results['F1 Score: Train'] = f1_score(y_train, predictions_train)\n",
        "\n",
        "        # Compute the Precision on Test Set\n",
        "        results['Precision: Test'] = precision_score(y_test, predictions_test)\n",
        "\n",
        "        # Compute the Precision on Training Set\n",
        "        results['Precision: Train'] = precision_score(y_train, predictions_train)\n",
        "\n",
        "        # Compute the Recall on Test Set\n",
        "        results['Recall: Test'] = recall_score(y_test, predictions_test)\n",
        "\n",
        "        # Compute the Recall on Training Set\n",
        "        results['Recall: Train'] = recall_score(y_train, predictions_train)\n",
        "\n",
        "        # Success\n",
        "        print(\"Training {} finished in {:.2f} sec\".format(learner.__class__.__name__, results['Training Time']))\n",
        "        print('----------------------------------------------------')\n",
        "        \n",
        "        final_results.append(results.copy())\n",
        "    # Return a dataframe of the results\n",
        "    return final_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfC--Apt-zcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Put Algorithms in Pipeline:\n",
        "# make a list of models\n",
        "models = [MultinomialNB(), DecisionTreeClassifier(), LinearSVC(), AdaBoostClassifier(), \n",
        "          RandomForestClassifier(), BaggingClassifier(),\n",
        "         LogisticRegression(), SGDClassifier(), KNeighborsClassifier()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2N1WP9K-7If",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re = pipeline(models, training_data, y_train, testing_data, y_test)\n",
        "results = pd.DataFrame(re)\n",
        "results = results.reindex(columns = ['Algorithm', 'Accuracy: Test', 'Precision: Test', 'Recall: Test', 'F1 Score: Test', 'Prediction Time',\n",
        "                          'Accuracy: Train', 'Precision: Train', 'Recall: Train', 'F1 Score: Train', 'Training Time'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSRnvf1zAROI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjBTKJ5JAGcl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bvwvLzc_cih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTUtNArv_WF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for hndling missing vlues\n",
        "def replace_missing_value(df, number_features):\n",
        "\n",
        "    imputer = Imputer(strategy=\"median\")\n",
        "    df_num = df[number_features]\n",
        "    imputer.fit(df_num)\n",
        "    X = imputer.transform(df_num)\n",
        "    res_def = pd.DataFrame(X, columns=df_num.columns)\n",
        "    return res_def"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}